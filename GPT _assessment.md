Assessment of the updated émile‑Kosmos repository (GitHub version)
Overview

The émile‑Kosmos repository is a living‑world simulator built around the Quantum Surplus Emergence (QSE) cognitive engine. An autonomous agent survives in a procedurally generated world by consuming food and water, avoiding hazards and crafting useful tools. The agent’s cognition is powered by a continuously evolving quantum wavefunction. The Shannon entropy of the wavefunction regulates the creativity of a local LLM, while a TD(λ) learner selects cognitive strategies. A structured tool registry constrains the action space, allowing the LLM or learned policies to choose from discrete actions such as move, examine, consume, craft and rest. When the LLM is unavailable, a heuristic fallback operates. The repository also vendored the emile‑mini cognitive engine and includes placeholders for advanced learning layers (GoalMapper and ActionPolicy) and multi‑agent extensions.

Compared with the zip version examined earlier, the GitHub repository has advanced significantly. The development roadmap in archive/ROADMAP.md shows that as of 4 February 2026 multiple phases and fixes have been completed: the GoalMapper and ActionPolicy layers are wired with teacher–student decay, a weather system and new objects have been added, world persistence implemented, multi‑turn LLM reasoning with handoff metrics is active, and a series of fixes (hazard avoidance, consciousness zones, slower teacher decay) and enhancements (visual field perception, richer internal state in prompts, event‑triggered narration, surplus‑faucet goal pressure, information metabolism and multi‑step LLM planning) have all been completed. Below is a detailed assessment of the current state, highlighting key components, improvements and future directions.

QSE cognitive engine and cognitive loop

Continuous quantum dynamics and entropy regulation: At every tick the surplus field S evolves via a split‑step Fourier method; the Shannon entropy of |ψ|² is computed and normalized to [0, 1]. High entropy indicates a spread‑out wavefunction and triggers high‑temperature (creative/divergent) behaviour, while low entropy indicates focused states and thus low‑temperature (convergent) reasoning. The TD(λ) module selects a cognitive strategy (explore/exploit/rest/learn/social) based on context, energy and reward history. The strategy also modulates the LLM’s personality (e.g., “curious and adventurous” for exploration or “efficient and focused” for exploitation) and filters which tool categories the LLM sees.

Integrated learning layers: The GitHub version wires in the GoalMapper and ActionPolicy layers. After the strategy is selected, the GoalMapper chooses an embodied goal (e.g., explore space, seek food, build shelter) based on energy, food availability, shelter and entropy. The ActionPolicy is a small neural network that learns to predict actions given a 30‑dimensional state vector; the teacher–student architecture decays the probability of using the LLM as the policy learns. Warm‑up has been increased to 500 samples and decay slowed (0.9995) to give the learned policy more time to stabilize, and performance‑gated decay ensures the student only takes over when its reward moving average matches or exceeds the heuristic’s. GoalMapper and ActionPolicy updates are invoked each tick.

Consciousness zones and survival override: The agent now classifies its state into zones based on energy and entropy. If energy < 0.25 the agent enters crisis; energy < 0.40 yields struggling; energy > 0.7 with entropy < 0.4 yields transcendent; otherwise it is healthy. In crisis the agent ignores the LLM and learned policy and uses a survival heuristic to flee hazards or consume resources. Struggling biases decisions toward the teacher (LLM); healthy uses the teacher–student balance; and transcendent allows more exploration and faster decay. These zones came from the Émile‑Cogito “consciousness ecology” concept and were recommended to prevent the learned policy from killing the agent. Thresholds have been raised relative to the earlier version (crisis at 25 % energy, struggling at 40 %).

Improved heuristic fallback: The heuristic now explicitly avoids hazards by scanning a two‑cell radius and moving away from nearby hazards. Emergency thresholds for food and water have been raised (seek food when energy < 0.45 instead of < 0.35; search radius increased to 8 cells; seek water when hydration < 0.4 instead of 0.3). The heuristic also crafts items, seeks shelter during storms and introduces stuckness detection to break cycles.

Event‑triggered LLM calls and asynchronous planning: Instead of calling the LLM every tick, the agent triggers LLM queries based on meaningful events—biome change, strategy change, consciousness‑zone transition, weather change, first discovery of an object type, high entropy, near‑death experiences or periodic refresh. LLM calls run asynchronously: a background thread requests either a single action or a multi‑step plan from the model. The agent maintains a plan queue and executes actions until the plan is complete or interrupted. Interrupt conditions include entering crisis, encountering hazards within two cells or satisfaction of specified re‑plan conditions such as “energy_critical” or “strategy_changed”. Multi‑step planning reduces LLM latency; planning is requested when no plan exists or the queue is nearly empty.

Surplus‑faucet goal pressure and information metabolism: The goal satisfaction variable averages recent rewards; if satisfaction is low, the agent’s metabolic cost slightly increases, encouraging it to pursue goals rather than idly wandering. The agent also tracks recent positions to compute novelty (exploration vs. camping). High novelty gives a small energy bonus while camping drains energy. These mechanisms derive from cognitive metabolic theories in Émile‑Cogito and help prevent camping on food piles.

Anti‑oscillation and stuckness detection: To avoid repetitive behaviour, the agent applies a penalty for repeating the same action too frequently (anti‑oscillation). A stuckness detector monitors recent positions and triggers context switches or random exploration if the agent remains within a small area.

World simulation and new mechanics

Biomes, objects and weather: The world is a two‑dimensional grid with biomes (plains, forest, desert, water, rock). Movement costs depend on biome; forests provide rest bonuses and resources; deserts increase energy cost and hazards spawn; water slows movement and provides hydration; rocks are impassable and spawn hazards. Day/night cycles and seasons remain, but the GitHub version adds a weather system with heat waves, storms and fog. Weather modulates metabolism (e.g., storms drain more energy in exposed biomes). New objects include herbs, seeds and planted crops, enabling planting and cultivation; seeds can be planted via a new plant tool. Food decays and migrates; water puddles dry up; hazards inflict damage that can be mitigated by crafted items like slings.

Crafting and farming: Craft recipes now include axe, rope, sling, bowl and new items like shelter frames (reduce night penalties), flint (cooking bonus) and basket (expanded inventory). Seeds can be planted to grow crops; herbs can heal. Crafting remains order‑independent and uses tags; the heuristic crafts when two recipe components are available.

Save/load persistence: The roadmap states that world persistence has been implemented. Although code details are not shown here, a save/load feature means the simulation can be paused and resumed without losing world state. This is crucial for long‑running experiments and for training policies across multiple episodes.

Multi‑agent preparation: While only one agent exists currently, the roadmap notes that multi‑agent support has been added for future experiments. The KosmosWorld class and rendering system are prepared to handle multiple agents and social strategies.

LLM integration and UI

Visual field perception and richer internal state: Instead of sending a simple text summary, the agent now builds an ASCII mini‑map of its 5×5 field of vision, including the agent’s position, nearby biomes, food (F), water (~), hazards (!), craft items (+) and biomes (T for forest, : for desert, ^ for rock). This mini‑map is delivered to the LLM along with a legend and textual summary of position, energy, hydration, weather and critical alerts such as low energy or dehydration. The prompt also includes internal QSE‑derived state (arousal, valence, consciousness zone) and feedback on the last action or current plan. This richer context allows the LLM to perform spatial reasoning (e.g., “move north to avoid hazard, then east to reach food”) and strategic planning.

Multi‑turn reasoning and narration: The OllamaReasoner now supports multi‑turn reasoning with maintained conversation history; its narrate function runs in a background thread and fires only on meaningful events (e.g., achieving a goal, near‑death experiences, weather changes). This reduces the queue of narration threads and produces more cohesive storytelling.

Strategic planning response format: For multi‑step planning, the LLM returns a JSON object containing an array of planned tool calls, a goal description and re‑planning conditions; interrupt rules (crisis, hazard proximity, energy critical, strategy change, inventory full, weather change) ensure the plan remains relevant. An example plan might move toward a forest, pick up wood, craft an axe and return for a crop—something that would previously require several LLM calls..

UI enhancements: The pygame renderer has been extended to display additional metrics such as the teacher probability, consciousness zone, novelty, goal satisfaction, stuckness flags and plan progress; weather and cultivation events are drawn on the world grid. Mouse interaction, minimap and dashboards remain on the roadmap, but the visual field map and internal metrics already enrich the player’s view.

Strengths and potential

The updated émile‑Kosmos repository demonstrates rapid progress toward a comprehensive cognitive playground:

Fully functioning cognitive loop with learning: The QSE engine, TD(λ) strategy selector, wired GoalMapper and ActionPolicy, multi‑step planning and asynchronous LLM integration form a three‑layer hierarchy. Heuristic reflexes handle emergencies; the learned policy handles routine decisions; the LLM acts as a strategic planner. Consciousness zones provide safety overrides for survival and enable deeper cognitive modulation.

Richer world mechanics: The addition of weather, farming and new items supports longer‑term survival strategies. Food decay and migration, energy/hydration management and crafting create non‑trivial resource dynamics.

Structured tools and learning‑friendly action space: The tool registry ensures actions are discrete and auditable, making it feasible to train reinforcement‑learning policies. Multi‑step planning further structures the LLM’s output.

Clear roadmap with phased integration: The project follows incremental phases, drawing inspiration from Émile‑Cogito while avoiding over‑complexity. Completed tasks are documented and verified, and upcoming tasks are ordered logically. Guiding principles (incremental integration, maintain tool‑based action space, test harnesses and performance profiling) emphasise maintainability.

Remaining challenges and next steps

Although substantial progress has been made, several areas warrant further attention:

Performance and latency: Even with asynchronous calls, LLM latency remains a bottleneck. Evaluate different models (e.g., phi3:mini vs. llama3.1:8b) and refine the trigger conditions for planning to balance world speed and cognitive depth. Consider caching repeated plan patterns or using smaller instruct models for high‑frequency decisions.

GoalMapper and ActionPolicy training: The wired learning layers require careful tuning. Monitor the teacher probability, reward EMAs and plan efficiency to ensure the learned policy overtakes the teacher gradually. Explore curriculum learning by adjusting world parameters (size, food density, weather frequency) to shape the policy’s experience. Use the new persistence system to run multi‑episode training sessions.

Multi‑agent and social behaviours: Phase 3 of the roadmap includes multi‑agent worlds and social learning. Start by allowing two agents to coexist; implement simple social tools (share food, teach actions) and observe emergent behaviours. Later, incorporate Cogito’s antifinity metrics to balance cooperation and competition.

Deeper Cogito integration: After stabilizing the baseline, selectively port concepts from Émile‑Cogito—such as autobiographical memory, qualia‐based sensorium and self‑sustaining ecology—into Kosmos. For example, autobiographical memory could replace the simple memory list and feed narrative context to the LLM; consciousness ecology could tie world richness to expressive success (e.g., new biomes spawn only when the agent narrates novel insights). These modules are complex, so integrate them gradually and benchmark the impact.

UI and user interaction: Implement a minimap and dashboards to visualise QSE metrics, reward curves and plan queues. Add mouse interactions to inspect cells or set waypoints. Provide configuration controls to adjust world size, speed, weather frequency and learning parameters at runtime.

Testing and documentation: Write unit tests for new behaviours (hazard avoidance, plan interrupts, persistence). Document the configuration parameters and runtime options in README.md. Provide sample datasets and logs to illustrate the agent’s learning progression. The Cogito notebook lacked documentation; avoid repeating this by keeping Kosmos thoroughly documented.

Conclusion

The GitHub version of émile‑Kosmos represents a major leap over the earlier zip version. By wiring in learning layers, introducing consciousness zones, enriching the world with weather and farming, adding visual perception and multi‑step LLM planning and implementing goal pressure and anti‑camping mechanisms, the project has moved from a reactive survival demo to a robust cognitive playground. The structure is modular and extensible, and the roadmap lays out a realistic path toward multi‑agent worlds and deeper Cogito concepts. Further work should focus on optimizing LLM latency, training and evaluating the new learning layers, and gradually integrating higher‑order cognitive modules. With careful engineering and documentation, émile‑Kosmos has the potential to become a powerful research platform for exploring embodied cognition, quantum‑inspired learning and LLM‑driven planning.