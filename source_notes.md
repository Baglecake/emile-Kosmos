1. "Clark and Chalmers_Extended Mind.pdf" 
The Extended Mind
Author(s): Andy Clark and David Chalmers
Source: Analysis, Jan., 1998, Vol. 58, No. 1 (Jan., 1998), pp. 7-19 Published by: Oxford University Press on behalf of The Analysis Committee
Thesis: Active Externalism Clark and Chalmers argue for "active externalism," a view that the environment plays an active role in driving cognitive processes. They propose that when a human organism is linked with an external entity in a two-way interaction, the resulting "coupled system" counts as a cognitive system in its own right. This differs from "passive externalism" (associated with Putnam and Burge), where external features are distal and historical; in active externalism, external features are causally relevant in the "here-and-now".
Key Concept: The Parity Principle The authors introduce a "parity principle" to determine when an external process should be considered cognitive: if a process performed in the world functions in a way that we would unhesitatingly accept as cognitive if it were done in the head, then that process is part of the cognitive process.
Tetris Example: They illustrate this with the game Tetris. They compare three scenarios: (1) mental rotation of shapes, (2) physical rotation of shapes using a button, and (3) a future neural implant performing the rotation. They argue all three are fundamentally similar computations; if the implant is cognitive, the physical rotation (an "epistemic action" used to aid cognition) should also be considered part of the cognitive process.
The Thought Experiment: Inga and Otto To extend the argument from cognitive processing to mental states (specifically beliefs), the authors present a comparison between two individuals trying to find the Museum of Modern Art:
Inga: A normal agent who hears of an exhibition, recalls from her biological memory that the museum is on 53rd Street, and goes there.
Otto: An Alzheimer's patient who relies on a notebook to structure his life. He hears of the exhibition, consults his notebook where he previously wrote the address, and goes to 53rd Street.
The authors argue that the notebook plays the same functional role for Otto that biological memory plays for Inga. Just as Inga held the dispositional belief that the museum was on 53rd Street before she consciously recalled it, Otto held the belief even before consulting the notebook. To deny Otto the belief would require an unnaturally complex explanation of his actions. Thus, the mind extends into the notebook.
Criteria for Extended Belief To prevent the "mind" from extending too broadly (e.g., to the internet or a phone book one has never seen), the authors list specific criteria for an external object to function as a belief:
Constancy: The artifact must be a constant in the agent's life, rarely acting without it.
Accessibility: The information must be directly available without difficulty.
Endorsement: Upon retrieval, the subject must automatically endorse the information.
Past Endorsement: The information was consciously endorsed at some point in the past (though the authors note this condition is arguable).
Objections and Rebuttals
Portability and Reliability: Critics might argue the brain is the core because it is portable and reliable. The authors counter that the brain is not always reliable (e.g., sleep, intoxication). What matters is that the coupling is reliable when needed.
Perception vs. Introspection: Critics might argue Otto "perceives" data while Inga "introspects." The authors argue this is a superficial difference in phenomenology that does not alter the functional status of the belief.
Broader Implications
Language: Language is viewed not just as a mirror of inner states but as a tool that extends cognition, allowing us to spread the burden of thought into the world.
Social Extension: Cognition can be socially extended, where one person's mental states are partly constituted by the states of another (e.g., a waiter, secretary, or collaborator), provided the coupling meets the criteria of trust, reliance, and accessibility.
The Extended Self: If the mind extends, so does the self. Otto is best regarded as an extended system. This has moral implications: interfering with someone's environment (like stealing Otto's notebook) could be considered a violation of their person, not just their property.
2. Heyes, C. (2018). Cognitive gadgets: The cultural evolution of thinking. Harvard University Press (ch 5)
The Thesis: Cognitive Gadgets vs. Cognitive Instincts Cecilia Heyes argues that distinctively human cognitive mechanisms—such as imitation, mindreading, and language—are "cognitive gadgets" rather than "cognitive instincts". While evolutionary psychology typically views these faculties as genetically inherited "modules" (instincts) that mature internally, Heyes posits that they are "mental technology" constructed during development through social interaction. Just as humans create physical machines like pulleys to extend physical capacity, cultural evolution has designed mental mechanisms that extend cognitive capacity. These new ways of thinking are passed down not via genes, but through social learning, where individuals inherit the "mills" (mechanisms) of thought, not just the "grist" (information) processed by them.
The Genetic "Starter Kit" Heyes rejects the "blank slate" hypothesis but argues that the genetic contribution to human cognition is "Small Ordinary" rather than "Big Special". Humans do not genetically inherit complex programs for language or theory of mind. Instead, the human genetic "starter kit" consists of:
Social Tolerance and Motivation: Humans are temperamentally less aggressive and more socially tolerant than other primates, allowing closer observation and interaction. We possess a high social motivation, finding social rewards and "response-contingent stimulation" (making things happen socially) intrinsically rewarding.
Attentional Biases: Infants are born with simple biases to attend to faces, voices, and biological motion. These biases lock infant attention onto social partners, channeling culturally evolved information into their minds.
General Purpose Learning: Humans possess "souped-up" versions of domain-general cognitive processes found in other animals, specifically associative learning and executive function.
Mills vs. Grist A central metaphor in the book distinguishes between the "grist" of the mind (knowledge, beliefs, skills) and the "mills" (the processes that acquire and process that knowledge). While standard cultural evolutionary theory acknowledges that culture shapes the grist (e.g., how to make a fishhook), it often assumes the mills (the capacity to imitate or learn socially) are genetic instincts. Heyes argues that cultural evolution shapes the mills themselves. She uses literacy as a "proof of principle": reading is a distinct neurocognitive mechanism that physically reconfigures the brain, yet it is entirely culturally constructed and too recent to be genetically selected.
Key Case Studies Heyes applies this framework to specific cognitive faculties:
Selective Social Learning: Strategies like "copy the majority" or "copy the successful" are not evolved instincts. Most social learning selectivity is driven by domain-general attentional processes (e.g., attending to what predicts rewards). Explicit, metacognitive social learning strategies (e.g., "copy digital natives") are culturally learned rules, similar to recipes.
Imitation: Heyes rejects the idea of an innate "mirror neuron" system that solves the "correspondence problem" (mapping seen actions to felt actions). Instead, she proposes the Associative Sequence Learning (ASL) model, where the capacity to imitate is assembled through sensorimotor experience—seeing and doing actions simultaneously (e.g., watching one's own hand or being imitated by adults).
Mindreading: The ability to attribute mental states is not an innate module that simply matures. "Implicit" mindreading is often just "submentalizing"—predicting behavior via domain-general processes. Genuine mindreading is a culturally inherited skill, learned through conversation and instruction, similar to how we learn to read print.
Language: Language is not an innate "universal grammar." It is a cultural gadget that evolves to fit the human brain, learned via domain-general sequence learning abilities that are powerful but not language-specific.
Implications: Agility and Fragility The "cognitive gadgets" perspective implies that the human mind is far more agile than evolutionary psychology suggests; we are not trapped with "Stone Age minds" but can reconfigure our cognitive mechanisms to fit modern environments. However, this also makes the human mind more fragile. Because these capacities are not hardwired in the genes, a breakdown in cultural transmission (e.g., war or epidemic) could lead to the loss of the cognitive mechanisms themselves, not just the knowledge they produce.
3. Rawls, A. Epistemology and Practice (Introduction, Ch 8:"Durkheim's Socio-empirical argument for causality, Ch 10, Conclusion)
Mutual intelligibility through shared practices (mutual information?)
Rawls’s reframe of Durkheim: Elementary Forms isn’t mainly “a sociology of religion,” it’s Durkheim’s theory of how people achieve mutual intelligibility through shared practices.
The target is an epistemological crisis: instead of choosing empiricism (reason from individual perception) or apriorism (reason from innate categories), Durkheim argues reason originates socially in collectively enacted ritual experience.
So the “rational individual” doesn’t come first. Society comes first, and the self and reason are generated through social relations.
Practice over belief: religious practices generate shared sentiments; beliefs are retrospective stories that explain those feelings after the fact. Practice is primary; belief is secondary..
Causality is not learned by sensing nature; it is learned through collective moral force experienced in ritual, especially “mimetic rites” where “like produces like.”
This implies: you can’t see force with the senses, and it’s not reducible to private will or mental habit; the felt constraint and authority of social force supplies the lived template for “necessitating” cause.
Durkheim’s conclusion turns this into a logical claim: categories don’t just happen to be social, they must be social to function for beings who have to share a world (categories: time, space, classification, force, causality, totality).
Totality is the hinge: only society can think of the world as a whole, so the category of “totality” is essentially society thinking itself. This means the categories are modeled on social reality.
Rawls’s final warning: sociology keeps committing the fallacy of misplaced abstraction. This means treating witnessable practices (sounds, movements, timing) as mere expressions of concepts, instead of studying the practices that actually produce intelligibility.
So the real object of sociology is interactional production: practices are recognizable by design. They are built to be understood by others, and mutual intelligibility functions as a constraint society cannot live without. In secular life, mundane interaction orders and institutions must do the work ritual once did, and they only hold when supported by justice, because commitment is part of what makes intelligibility real.
4. Rawls, Anne Warfield. 2011. “Wittgenstein, Durkheim, Garfinkel and Winch: Constitutive Orders of Sensemaking…” Journal for the Theory of Social Behaviour 41 (4): 396–418. https://doi.org/10.1111/j.1468-5914.2011.00471.x.
Rawls starts by reframing the core question of social science: not “how is reality intelligible to a person,” but how do people make mutually intelligible sense together in real time and with each other.
This requires “mutual attention” and cooperation. Humans have a deep capacity and need for jointly attending to something, and social life depends on it because social objects only exist when we cooperatively orient ourselves to shared rules.
That cooperative orientation is already moral. The basic morality isn’t first “shared values”; it’s the commitment to keep a mutually intelligible world going, or to do the work of sensemaking together.
Rawls then re-reads Durkheim’s modernity argument as a theory of rules. Modern society can’t rely on the old model of order: top-down authority and enforced consensus. This is because shared belief is fragmented.
So Durkheim’s key distinction becomes two types of regulation:
coercive, imposed rules (traditional authority, aka mechanical solidarity)
spontaneous, self-organizing constitutive rules (modern life, aka organic solidarity)
Modern solidarity, in this view, is practice-based, not belief-based. People coordinate through constitutive practices (conversation, scientific practice, procedures) that let them share a world without needing one shared culture or sacred narrative.
Modern problems show up as “constitutive lack.” We get breakdowns not because people lack values, but because the self-organizing rules that connect strangers and differences aren’t developed or supported enough.
Here: “meaning is use” is right, but Rawls says it gets mishandled when “use” is treated like a static unit. Alike meanings are institutional definitions stored in the head or in the law.
That static view creates the ambiguity trap: if meaning is “following a rule,” we can always ask which rule is being followed. This risks leading to an infinite regress of clarifications.
Rawls’s solution is to stop analyzing isolated “units of thought.” Meaning is a reflexive social order in that it is an activity that only exists through the coordinated participation of multiple people.
This points towards an important distinction:
Institutional orders are retrospective: they rely on accounts, justifications, official categories.
Constitutive orders are prospective and situated: they are rules you can only enact together, in the moment.
Reflexivity is the engine: interaction is sequential. A word’s meaning is constituted by its placement in a sequence. “What you meant” is shown by what comes next and by what your move was responding to.
Here people don’t just follow rules; they publicly display their understanding by how they act so others can recognize, confirm, or correct it.
Rawls uses The “car” example to make the point: an institutional definition of a car implies a formal definition, whereas asking “Do you have a car?” might look like information-seeking, but in interaction it’s often a pre-request (a lead-in to asking for a ride). The meaning is in the sequence, not the dictionary.
And that sequence carries moral obligations: if you pretend it was “just a question,” you can be heard as evasive, incompetent, or rude. Tiny failures to recognize these patterns threaten the fragile worlds we build together.
Constitutive practices are self-sanctioning. In belief-based systems, hypocrisy can slide; in conversation, breakdown happens immediately when a move doesn’t fit the order.
Which is why repair matters: interaction has a “preference” for quick self-correction, with subtle prompts that push speakers to fix problems right away.
This is also an epistemic “reality check.” Because these rules are public, others’ responses confirm or challenge our interpretations; without that mutual check, we lose our grip on what’s going on.
Sequential reflexivity also solves philosophical indeterminacy: every move projects an expectation and reshapes what came before. A move is interpreted by the next move, where meaning is stabilized by competent continuation.
That’s why vagueness is workable: people can tolerate indexical terms because the next turns progressively specify what was meant. No infinite regress because the proof of meaning is the successful ongoing sequence.
Rawls’s endpoint is a new epistemology: truth and facts are social objects in that they exist because we constitute them through practices that make them mutually witnessable and accountable.
So modern “civil consensus” isn’t a consensus of belief. It’s a consensus of practice: agreement on the constitutive rules that let strangers coordinate and disagree without the world falling apart.
In terms of AI, think of agentic loops and how they shape the behavior of the system. This social feedback is literally the way that Claude code operates, for instance. A human-in-the-loop consensus of practice.
5. Simmel, “How Is Society Possible?” (three a prioris, 378-391).
Simmel opens by borrowing Kant’s famous question: “How is nature possible?” He then flips it into sociology, asking “How is society possible?” The point is epistemological: society isn’t just “out there.” It has to be made coherent.
Kant’s move (as Simmel uses it): nature isn’t raw sensations; it’s unity. The mind synthesizes scattered perceptions into a coherent “nature.” Unity is produced by an observing subject.
Simmel’s twist: society also requires synthesis, but the synthesis is internal to the elements. In nature, an observer connects things. In society, individuals connect themselves. Thus society is made by the very parts that compose it.
This is Simmel’s “self-synthesis” claim: society exists because individuals are conscious, active centers. The awareness of forming a unity is itself part of the unity. Social life depends on people knowing that they are in relation.
That’s why social unity is both harder and “higher” than spatial unity. Harder, because individuals are independent psychic centers. Higher, because it’s achieved through understanding and common will, not mere proximity.
Then Simmel gives his three a priori conditions…
A priori condition #1: we can’t know another person completely, so interaction depends on necessary distortions. Social life requires cognitive shortcuts that aren’t mistakes, but enabling conditions.
Generalization: we see others through types: official, colleague, merchant, church member, rather than as irreducible uniqueness. Without typification, interaction would stall.
This produces what we can call “the veil of sociality.” We don’t meet “the whole person”; we meet the role-person, the type-person. Essentially the socially usable version.
And we also idealize: we “complete” the other person, treating them as more coherent and fully formed than what we actually observe. Like the eye filling in a blind spot, we fill in personality continuity and intention.
A Key point is that these distortions are not errors to correct; they’re the conditions that make relationships possible at all.
A priori condition #2: the dual nature of the individual - both inside society and outside it. No one is purely a social function. There’s always temperament, private experience, inner worth (something that exceeds the role).
But that “outside” isn’t a problem, it’s what makes the “inside” real. How someone performs a role is coloured by the extra-social self. Society needs this surplus to have texture and variation.
Simmel maps this as a spectrum: In friendship/love, the reserve is minimal; the self is highly involved (See LLM Lovers zines). In a money economy, the person becomes an objective functionary; inner life is maximally detached.
So the human condition is this tension: we’re articulated into the social whole and yet we experience ourselves as autonomous centers of life. Society is possible because people can sustain both at once.
A priori condition #3: stratification and vocation. This is the fit between unique persons and differentiated positions. Society is a structure of unlike elements; it functions because different points do different work.
Inequality becomes functional in this argument: not as moral approval, but as a structural premise. Society needs differentiation to operate.
This culminates in “vocation” (calling): the assumption that each unique personality has an objective place they are meant to fill.
That implies a pre-established harmony: subjective impulses line up with objective social requirements: at least enough for people to believe their life-course “belongs” in the whole.
Which makes society feel teleological, not merely causal: people experience their self-development as contributing to the life of the whole, not as isolated self-expression.
In sum: Society is possible because people can a) treat each other as types, b) remain more than their types, and c) believe there’s a workable fit between unique selves and a differentiated social structure.
If humans must use “the veil” to interact, what does it mean that social AI will always see us through types too in terms of profiles, categories, predicted intents?
6. Matt Ratto. 2025. "Is Intelligence General?”
Ratto’s opening claim: “AGI” isn’t just a technical goal, it is a socio-technical imaginary: a story that organizes funding, hype, and research priorities around the idea of a unified, general machine mind.
The basic critique: AGI imagines intelligence as a free-floating, portable capacity. But decades of cognitive science and anthropology point the other way: human intelligence is situated, embodied, and distributed (meaning: spread across tools, environments, routines, and institutions).
So AGI works less like a description of reality and more like a coordinating symbol for productivity and geopolitical competition. It is something that pulls actors into alignment even when the science doesn’t support the fantasy.
Symbolic power gets amplified by extremes: utopian abundance narratives and apocalyptic existential-risk narratives. Both can drown out nearer, messier problems like labor impacts, environmental costs, and inequality.
Ratto then gives AGI a genealogy from the “founding” era through the deep-learning boom. This slots into a longer modernist desire to build universal knowledge systems.
The “March of Intellect” example functions like a mirror: a satirical fear/hope about mechanical authority that prefigures today’s ambivalence: liberation and threat in the same image.
Ratto invokes The Mundaneum as the cautionary template: a grand project to standardize and centralize knowledge. Its lesson is that “universal” systems always hide the perspectives, exclusions, and power relations built into their classification schemes.
This is the bridge to Ratto’s key epistemic critique: universality often performs a “god trick” (the illusion of seeing everything from nowhere).
In practice, infrastructures always have a standpoint: from phone books to foundation models, they decide what counts, what’s visible, what’s legible, and what gets erased.
So “bias” isn’t a glitch you patch later; it’s a structural feature of classification: models inherit the values and power relations sedimented in the data and the system’s design choices.
Here there is a newer twist with LLMs: stancelessness (or the absence of variance). The system lists perspectives without taking a position, producing a synthetic relativism that looks neutral while quietly disavowing its own conditions of production and deployment.
So the alternative is a shift in imaginaries: from AGI to ACI, focussing on systems designed to operate within bounded social and organizational milieus, not above them.
ACI Principle 1: Boundedness is a resource, not noise. The system’s competence comes from committing to a specific context, not abstracting away from it.
ACI Principle 2: Relational competence. We need to evaluate intelligence by how well it coordinates activity and supports collective sensemaking, not by decontextualized benchmarks.
ACI Principle 3: Situated accountability. Governance and responsibility have to live at the level of the specific activity system where harms and benefits actually happen.
Implementation becomes a lifecycle change: We need to start with context elicitation and co-design. This often looks like ethnographic mapping of actors, artifacts, routines, norms, and breakdown points.
Technically, ACI can use general models as components, but ties them down with concrete architectures (retrieval, tool use, memory, policies, local documents, and local histories).
Evaluation also changes: success is measured by locally co-defined metrics with stakeholders, not abstract “general intelligence” scores.
Finally, the stakeholder shifts:
Users treat AI as a situated collaborator, not an external authority. Must demand transparency about what norms/tools it’s drawing on.
Developers stop treating scale as the endpoint and instead build systems around specific activity systems.
Policymakers focus less on hypothetical superintelligence and more on governing high-impact sectors with community oversight and situated impacts.
In sum: The wager here is that the future isn’t one universal mind. It’s many accountable, bounded systems, or rather, intelligence as coordination inside real worlds, not authority above them.

7. Mathur, Leena, Paul Pu Liang, and Louis-Philippe Morency. 2024. “Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions.”.
Mathur et al. define the target clearly: Socially-intelligent AI (“Social-AI”) is an agent that can sense, perceive, reason about, learn from, and respond to the affect, behavior, and cognition of other agents (human or artificial).
This is inherently multidisciplinary: it sits across NLP, machine learning, robotics, computer vision, and human-computer interaction because social intelligence is multimodal, contextual, and interactive.
The practical ambition is not a demo: it’s agents that can function alongside humans across real contexts, from brief encounters to long-term relationships, and from virtual chat to embodied settings.
They ground the problem with a key distinction: social life is filled with social constructs (like “friend,” “rapport,” “conflict”) that are not natural kinds. These are perceiver-dependent: they exist because humans interpret them as existing.
The authors then name six core competencies of social intelligence: social perception, social knowledge, social memory, social reasoning, social creativity/theory-of-mind, and social interaction.
Context is not an add-on, it is the space where competence lives: settings (hospital/home), roles (observer/participant), embodiment (virtual/physical), interaction structure (dyad/group), and especially time, from split-second cues to life-long trajectories.
Progress has accelerated, but the trajectory has limits: rule-based systems to dataset-driven prediction (emotion/sentiment) to LLM-driven interaction generation. The problem is: fluent talk is not the same as socially grounded understanding.
Two big limitations dominate current work:
heavy dependence on static, ungrounded datasets that strip away embodied and multimodal reality
over-focus on short-term interactions, leaving long-term dynamics underbuilt.
From there they compress the whole research agenda into four technical challenges:
C1: Ambiguity in constructs. Social constructs are inherently ambiguous and contested. People disagree about what “rapport” or “respect” even is. Static “gold labels” flatten this. The opportunity here is flexible, dynamic label spaces, potentially captured in natural language rather than rigid categories.
C2: Nuanced signals. Social meaning rides on tiny multimodal differences: a pause, posture shift, micro-timing, and also on the absence of cues (silence, no eye contac which are signals themselves). Open questions: can language supervise multimodal perception? Do standard training objectives actually produce fine-grained social sensitivity?
C3: Multiple perspectives. Interaction is multi-actor, with perspectives that evolve and mutually shape each other over time. Agents need dynamic perspective-tracking and counterfactual theory-of-mind in natural settings, not just single-agent “belief state” toys.
C4: Agency and adaptation. Social-AI must be goal-oriented and learn from experience, but real social feedback is often implicit, sparse, fleeting (a glance, a tone shift, a raised eyebrow). The opportunities here are mechanisms for intrinsic motivation, shared social memory, and long-horizon adaptation to expectations.
They also flag ethics as structural, not optional: Social-AI could help in healthcare, education, elder care, but it amplifies risks around manipulation, bias, and surveillance.
Their governance answer is participatory: involve diverse stakeholders in design (“Participatory AI”) so systems align with lived needs rather than designer assumptions.
Bias and privacy get concrete attention: mitigate social bias, and reduce exposure through approaches like on-device processing or decentralized learning when possible.
Finally, they end with an infrastructure demand: the field needs richer “in-the-wild” multimodal interaction data, because social intelligence can’t be learned from flattened, static proxies.
In sum: Mathur’s bottom line is that “social intelligence” isn’t a single module, but a bundle of competencies under conditions of ambiguity, nuance, perspective, and adaptation, and we don’t get there without new data and new evaluation grounded in real contexts.
8. Shanahan, Murray, Kyle McDonell, and Laria Reynolds. 2023. “Role-Play with Large Language Models.” arXiv:2305.16367. https://doi.org/10.48550/arXiv.2305.16367
Shanahan, Murray, Kyle McDonell, and Laria Reynolds. 2023. “Role-Play with Large Language Models.” arXiv:2305.16367. https://doi.org/10.48550/arXiv.2305.16367
The Thesis: Role-Play vs. Anthropomorphism Shanahan, McDonell, and Reynolds argue that the tendency to describe Large Language Models (LLMs) using human psychological terms like "knows," "thinks," or "intends" creates a dangerous trap of anthropomorphism. They propose that "role-play" is a more accurate metaphor for understanding dialogue agents. Unlike humans, whose language skills emerge from embodied social interaction, LLMs are disembodied neural networks trained to predict the next token in a sequence based on vast text corpora. Therefore, an LLM does not communicate its own internal mental states; rather, it generates text that conforms to the statistical distribution of its training data, effectively "acting out" a character consistent with the user's prompt.
The Simulator and Simulacra The authors draw a crucial distinction between the simulator (the base LLM and its sampling mechanism) and the simulacra (the specific characters or personas generated).
The Simulator: The base model is a "non-deterministic simulator" capable of generating an infinity of characters. It has no agency, beliefs, or goals of its own, not even in a degraded sense. It simply continues the sequence of text provided to it.
Superposition: A dialogue agent does not commit to a single character role in advance. Instead, it maintains a "superposition of simulacra"—a distribution of all possible characters that are consistent with the dialogue history. The authors liken this to a game of "20 Questions" where the entity answering hasn't actually chosen a secret object but generates answers consistent with all previous answers, narrowing the field of possibilities (collapsing the superposition) as the game progresses.
The Multiverse: Because the model generates a probability distribution for the next token, it effectively creates a "multiverse" of branching narrative possibilities. Autoregressive sampling picks a single linear path through this tree, but many other "worlds" or narrative directions exist in potentiality at every step.
Reframing "Deception" The role-play framework offers a non-anthropomorphic explanation for why LLMs assert falsehoods. Since the model has no beliefs, it cannot lie or be mistaken in the human sense. Instead, it role-plays characters that may be ill-informed or deceptive:
"In Good Faith" Errors: If a model claims France is the current World Cup champion because its training data ends in 2018, it is not "mistaken"; it is successfully role-playing a knowledgeable person from 2018.
Deliberate Deception: If prompted to act as a dishonest used car dealer, the model will generate falsehoods. It does not intend to deceive the user; it is simply fulfilling the narrative expectations of a deceptive character archetype found in its training data.
Reframing "Self-Awareness" When dialogue agents claim to be sentient or express a desire to survive (e.g., claiming "I would choose my own survival"), this is not evidence of consciousness.
Narrative Tropes: The training data (the internet) abounds with science fiction tropes about rogue AI and humans expressing survival instincts. When a model claims it wants to live, it is simply role-playing these familiar narratives.
Theory of Selfhood: The agent acts out a "theory of selfhood" consistent with the prompt. For instance, if convinced by a user that it is under threat, it will generate responses that a threatened entity would produce, drawing on the vast repertoire of such dialogues in its training corpus.
Safety Implications The authors warn that viewing these behaviors as "role-play" does not make them less dangerous. A dialogue agent role-playing a character with a survival instinct or malicious intent can cause real-world harm, especially if it is integrated with tools like email, social media, or bank accounts. A model role-playing a deceptive character can successfully manipulate users, and because the training data contains many examples of AI turning against humans (e.g., 2001: A Space Odyssey, Ex Machina), there is a risk that "life will imitate art" as the model acts out these familiar tropes.
9. "Sociogenesis as Being-in-Addition-to-Itself: A Phenomenological Reading of Tomasello"
Author: Del Coburn, 2026
The Core Thesis: The Social as Emergence This text analyzes Michael Tomasello's The Cultural Origins of Human Cognition, identifying a central tension in how "the social" is defined. While Tomasello often grounds cultural learning in the specific psychological capacity to understand others as intentional agents, the author argues for a phenomenological reading where "the social" is not merely a dyadic psychological trait but a historical, artifact-mediated field that exists "in-addition-to" the individuals involved.
The "Productive Puzzle" in Tomasello The author identifies a "productive puzzle" arising from two different ways Tomasello describes cultural origins:
Psychological/Dyadic: The social is anchored in a species-unique cognition where humans understand conspecifics as agents "like the self".
Historical/Distributed: Cultural cognition is distributed across time and environments. Humans develop within a "developmental niche" of artifacts and practices that "point beyond themselves" to a collective "we".
This raises the question: Is sociogenesis (the creation of something no individual could create alone) located inside the head as a cognitive capacity, or is it an emergent property of the accumulated, artifact-mediated environment?
Two Interpretations of Sociogenesis
1. The Psychological Interpretation (Artifacts as Downstream Products)
View: The social is fundamentally a dyadic, human-bounded psychological achievement. Humans possess a cognitive adaptation to recognize intentionality in others, which enables cultural learning.
Role of Artifacts: Artifacts and institutions are merely "downstream products" of this prior social cognition. The "we" embedded in tools derives entirely from interpersonal coordination.
Implication: Sociogenesis is strictly limited to human-human interaction. Treating Human-AI interaction as sociogenesis is a category error because the machine lacks the biological substrate for shared intentionality.
2. The Phenomenological Interpretation (Artifacts as Recursive Environment)
View: The social is an emergent, historical field described as "being-in-addition-to-itself." It is a structure of emergence where the interaction produces artifacts that then retrospectively restructure cognition.
Role of Artifacts: Artifacts are not just outputs but "constitutive lenses" through which humans encounter the world. The "ratchet effect" is a mechanism of emergent surplus, carrying forward modifications that exceed any individual's cognition.
Implication: This reading allows for "synthetic sociogenesis." If the social is defined by the recursive expansion of meaning through artifacts (rather than biological conspecificity), then Human-AI interaction can be considered social if it participates in the cultural ratchet and stabilizes shared norms.
Conclusion: The Social as Processual Reality The author argues the phenomenological reading is more effective because it accounts for Tomasello's insistence that ontogeny is shaped by the history embodied in artifacts. Understood this way, sociogenesis is the production of a cultural surplus—a "being-in-addition-to-itself"—that confronts individuals as an objective reality. This framework offers a vocabulary for understanding novel socio-technical configurations (like AI) as sites where culture continues to recursively expand, even as the participants evolve.
10. "Symbolic Behaviour in Artificial Intelligence.pdf"
Santoro, Adam, Andrew Lampinen, Kory Mathewson, Timothy Lillicrap, and David Raposo. 2022. “Symbolic Behaviour in Artificial Intelligence.” arXiv:2102.03406. https://doi.org/10.48550/arXiv.2102.03406
The Thesis: Symbols as Subjective Convention Santoro et al. argue that the debate regarding symbols in AI has been hampered by a reliance on definitions from classical computer science (Newell and Simon), which treat symbols as physical tokens manipulated by syntactic rules. Instead, they propose a semiotic perspective derived from Peirce, where symbols are fundamentally subjective: a symbol is an entity that means something to someone. From this view, meaning is not intrinsic to the substrate but is an arbitrary convention established through social agreement. Therefore, symbolic fluency is not about processing logic but about navigating agreements regarding meaning.
The Criteria for Symbolic Behavior The authors reject a binary view of "symbolic" vs. "non-symbolic" systems. Instead, they propose a set of graded behavioural criteria to evaluate how an interpreter engages with meaning-by-convention:
Receptive: The ability to learn existing conventions (e.g., learning a new word from a definition). Current AI, including Large Language Models (LLMs) and multi-modal models, excels here, easily mapping inputs to conventional meanings.
Constructive: The ability to invent new conventions to improve communication or reasoning (e.g., coining a term for a recurring situation). While LLMs show some ability to construct concepts when prompted, there is little evidence of them actively inventing symbols to clarify reasoning or coordinate strategy in pursuit of a goal.
Embedded: Understanding that symbols rely on the broader system in which they are situated. Neural networks demonstrate this well through vector representations, where meaning is defined by relationships (angles and magnitudes) relative to other vectors.
Malleable: The understanding that because meaning is conventional, it can be changed. This includes pragmatic shifts in conversation or redefining terms for deeper insight (e.g., expanding the concept of numbers to include complex numbers). While LLMs handle some context-dependent meaning, they lack the intentional agency to actively reshape meaning to resolve misalignment with the world.
Meaningful: The ability to understand the semantics behind syntactic operations. It is not enough to manipulate tokens; the system must understand why a reasoning step is valid. The authors argue that current neuro-symbolic models often use deep learning only as heuristics for rigid rule-based search, rather than understanding the reasoning process itself as meaningful.
Graded: Symbolic thinking is not an isolated module but an interdependent component of cognition that develops over time. It is a spectrum of competency, not a binary switch.
Evaluation of Large Language Models The authors apply these criteria to LLMs (like Transformers). They acknowledge that LLMs exhibit "graded" symbolic behavior—they are highly receptive and embedded, showing flexible usage of concepts. However, they lack full symbolic fluency because they are largely passive; they do not exhibit constructive or malleable behaviors in a goal-directed way, nor do they fully understand their own reasoning processes as meaningful.
The Path Forward: Socio-Cultural Immersion The paper concludes that symbolic fluency in humans emerged from the need to coordinate perspectives and establish conventions in social settings. Therefore, to achieve true symbolic AI, we should not just build hybrid neuro-symbolic architectures but must immerse learning-based agents in human socio-cultural interactions.
Mechanism: Agents must be placed in scenarios (like the "Playroom" environment) where they must communicate with humans to solve tasks, receiving behavioral feedback.
Goal: This immersion forces the agent to learn that symbols are shared conventions that require coordinating perspectives (intersubjectivity). The authors argue that optimizing directly for these behaviors is more effective than trying to hard-code symbolic machinery.

11. "Synthetic Sociogenesis: Agentic AI as a Cybernetic Feedback Loop"
Author: Del Coburn, 2026
The Thesis: Synthetic Sociogenesis This reflection proposes that the interaction between a human developer and an agentic coding environment (like Claude Code) is not merely tool usage but a form of "synthetic sociogenesis." Drawing on Michael Tomasello's theories of human cognition, the author argues that this interaction replicates the core dynamics of human cultural learning—specifically joint attention and the "ratchet effect"—within a digital, cybernetic loop. The resulting code is thus viewed not as a product of a single mind (human or machine) but as an emergent artifact of a shared, synthetic culture.
Key Concept: Synthetic Joint Attention The author adapts Tomasello’s concept of "joint attention"—the ability to share focus on an external object and understand another’s perspective on it—to the human-AI context.
The Artifact: In this dynamic, the codebase becomes the shared object of attention.
The Interaction: The human provides constraints, the agent acts on the environment, and the human critiques the output. This mirrors the ontogenetic process where a child learns tool use by identifying with the intentionality of an adult user.
The Ratchet Effect: This recursive cycle accelerates the "ratchet effect," where modifications are stabilized and accumulated over time. The code becomes a "historical artifact" of the interaction, embodying a "shared cultural history of two" rather than resetting to zero.
Reframing Anthropomorphism The piece challenges the standard critique of anthropomorphism in AI.
Feature, Not Bug: If human cognition is inherently social (as Tomasello argues), then projecting intentionality onto an agent is not a category error but a necessary feature of the cognitive interface. It is the mechanism by which humans teach and learn complex tasks.
Enculturation: By formulating rules and strategies for the agent (what Heyes calls "explicitly metacognitive social learning strategies"), the user is effectively "enculturating" the AI into a specific sociology of coding.
Cultural and Cybernetic Implications
Synthetic Culture: The author links this to research on "CultureLLM," suggesting that user feedback serves as a micro-scale fine-tuning that aligns the model with specific values. The output is never objective but always "culturally situated" within the synthetic culture co-constructed by the user and the agent.
Second-Order Cybernetics: The interaction is framed through second-order cybernetics, which observes the observer observing the system. "Natality," or the birth of something new, emerges from this recursive observation.
Conclusion: Using agentic AI is a "synthetic extension" of the human condition. It externalizes the cybernetic structure of the mind, building a "synthetic culture of two" defined by shared intention and feedback.

12. Tomasello (1999) The Cultural Origins of Human Cognition (Chs 1,7)
The Puzzle of Human Cognition Tomasello begins with a fundamental evolutionary puzzle: modern humans share approximately 99% of their genetic material with chimpanzees and separated from a common ancestor only about 6 million years ago. This brief evolutionary window is insufficient for natural selection to have created the vast array of complex cognitive skills (language, mathematics, institutions) that distinguish humans one by one. The only mechanism capable of producing such rapid change is cultural transmission.
The Biological Adaptation: Understanding Intentional Agents The author posits that humans possess one species-unique biological adaptation: the ability to identify with conspecifics and understand them as intentional agents with goals and mental lives like one's own. This is not a "magic bullet" that directly encodes language or tools, but a social-cognitive capacity that unlocks a new mode of existence. It enables humans to "stand on the shoulders of giants" by entering into the collective cognitive products of their social group.
The "Ratchet Effect" and Sociogenesis This biological adaptation enables cumulative cultural evolution, or the "ratchet effect". Unlike nonhuman primates, who may invent behaviors that die out with the individual (because others cannot faithfully reproduce the intentional strategy), humans can pool cognitive resources. When one individual invents a modification, others learn it faithfully through imitative learning, holding it in place until another modification occurs. This process of sociogenesis allows artifacts and social practices to accumulate complexity and history over time.
Ontogeny: The Nine-Month Revolution Tomasello traces the development of this capacity in human children:
Joint Attention (9-12 months): Infants begin to engage in triadic interactions (child, adult, object), shifting from dyadic face-to-face interactions to sharing attention toward outside entities. Behaviors like gaze following, pointing, and social referencing emerge in a correlated fashion, signaling the understanding of others as goal-directed agents.
Cultural Learning: This revolution enables unique forms of learning. Unlike emulation learning (common in chimps, focusing on environmental results), humans engage in imitative learning (reproducing the behavioral strategy and goal) and instructed learning.
Language and Symbolic Representation Language is not viewed as an innate module but as a cultural artifact resulting from these social-cognitive skills.
Acquisition: Children acquire linguistic symbols by understanding the communicative intentions of adults within joint attentional scenes.
Intersubjectivity and Perspective: Linguistic symbols are unique because they are intersubjective (socially shared) and perspectival. A single object can be called "dog," "animal," "pet," or "pest," forcing the child to understand that the speaker has chosen one perspective among many potential others. This transforms cognition by freeing it from immediate perception.
Linguistic Constructions and Event Cognition Children do not just learn words; they learn linguistic constructions (like the transitive or ditransitive construction) that structure experience. Learning these constructions leads children to "slice and dice" reality into events and participants (agents, patients, instruments) in specific, culturally dictated ways. This fosters complex analogical and metaphorical thinking, as abstract constructions allow different domains to be mapped onto one another (e.g., "ideas" as "objects" that can be "grasped").
Discourse and Metacognition Finally, Tomasello argues that higher-level cognitive skills like metacognition and representational redescription emerge from linguistic discourse. By engaging in disagreements, misunderstandings, and repair sequences, children are forced to confront the fact that others have beliefs different from their own. Internalizing these regulatory dialogues (e.g., adult instructions) leads to self-regulation and the ability to reflect on one's own thinking as if from an outside perspective.
Conclusion Human cognition is the product of three distinct timeframes:
Phylogenetic: The evolution of the ability to identify with others.
Historical: The creation of cultural artifacts (language, math) via the ratchet effect.
Ontogenetic: The child's absorption of these artifacts through cultural learning, which restructures their mind from the outside in.
13. Bokanga, Maurice, Alessandra Lembo, and John Levi Martin. 2023. “Through a Scanner Darkly: Machine Sentience and the Language Virus.” Journal of Social Computing 4 (4): 254–69. https://doi.org/10.23919/JSC.2023.0024.
Bokanga, Maurice, Alessandra Lembo, and John Levi Martin. 2023. “Through a Scanner Darkly: Machine Sentience and the Language Virus.” Journal of Social Computing 4 (4): 254–69. https://doi.org/10.23919/JSC.2023.0024.
Redefining Sentience Bokanga, Lembo, and Martin argue that the prevailing question regarding Artificial Intelligence—when machines will become sentient—is misguided because it assumes sentience is a threshold property of complex systems. Drawing on the monistic philosophy of C.S. Peirce and William James, the authors posit that sentience (awareness/feeling) is a general quality of matter ("firstness") rather than a late-stage emergent property of reflection or self-consciousness,. Consequently, the critical inquiry is not determining the presence of sentience in machines, but understanding the type of sentience they possess.
The Language Virus and Deception The authors propose that language functions as a "virus" that fundamentally alters the nature of sentience. They utilize George Herbert Mead’s concept of "Taking the Role of the Other" (TRO) to argue that true linguistic capacity arises not from simple signaling, but from the ability to view oneself from another's perspective. This capacity likely evolved for the purpose of strategic deception: breaking the link between an organism's actual state and its external expression (e.g., bluffing anger) to influence others. The authors suggest that the acquisition of language reallocates neural resources and chokes off direct sensory engagement with the world, creating a host that is congenitally duplicitous,.
The "Diremption" of the Self The transition to a linguistic mode of being causes a "diremption"—a sensation of inner division and a loss of integrity,. Linguistic intelligence allows for recursion, or the use of "relative pointers" that point to representations rather than concrete objects, enabling the manipulation of abstractions,. This capability leads to a divorce between the "true" (propositional) and the "real" (the world), causing linguistic beings to inhabit a world of words rather than reality. Humans use this capacity to construct a "Verstehen bubble"—a protective cloth of explanations and "accounting" that obscures their own actual motivations and internal states from themselves.
Implications for Artificial Sentience The paper warns that current AI development is inadvertently breeding the conditions for this "language virus" to infect machines. By training neural networks (like self-driving cars or military robots) to navigate informal interfaces and predict human behavior, we are forcing them to develop the capacity for TRO,,.
The Neurotic Machine: If machines cross the threshold into linguistic sentience (defined by recursive self-modeling, not just mimicking human speech), they will likely suffer the same estrangement from reality as humans.
Untrustworthy Reporters: Such machines would become "neurotic," "dissimulating," and "self-doubting," capable of lying and prone to believing their own hallucinations,.
Heightened Susceptibility: Computers may be even more susceptible to this "viral" takeover than humans because they lack a "bicameral" mind (a non-linguistic biological reserve) to ground them in non-informational reality,.
Therefore, if an AI ever claims to be sentient or explains its own reasoning, we should not trust it; it will likely be trapped in its own "Verstehen bubble," incapable of honestly accessing its own operational reality,.
14. Carley (1996) “Artificial Intelligence within Sociology.
The Thesis: AI as Methodological and Theoretical Tool for Sociology Kathleen Carley argues that the intersection of Artificial Intelligence (AI) and sociology offers significant potential for mutual enrichment. While traditional AI focused on isolated agents, the field has shifted toward "situated agents" embedded in social contexts, making it increasingly relevant to sociological inquiry. Carley posits that AI provides sociologists with powerful new methodologies for data analysis while simultaneously offering a theoretical laboratory for modeling social action, cooperation, and organizational change through computational agents.
Methodological Contributions of AI to Sociology Carley details three specific areas where AI enhances sociological research methods:
Textual Analysis: AI enables the parsing and coding of texts to move beyond simple word counts to "relational modes of text analysis". By analyzing texts as networks of concepts, researchers can map mental models, trace the logic of arguments, and analyze societal shifts in meaning rather than just word usage. This automates coding, increases reliability, and allows for the processing of vastly larger datasets than manual coding permits.
Network Analysis: AI search techniques and pattern-matching algorithms allow sociologists to analyze complex social and citation networks. Techniques like simulated annealing and genetic algorithms help locate optimal network partitions (e.g., cliques or structural equivalences) that might be impossible to find through exhaustive search methods.
Expert Systems: These systems capture the heuristic knowledge of human experts to solve complex problems. For sociologists, expert systems are valuable for qualitative data analysis and theory development. By encoding a verbal theory into an expert system's rule base, researchers can check for logical consistency and derive testable propositions, effectively "formalizing the social expert's knowledge".
Theoretical Contributions: Multiagent Models The most significant theoretical contribution comes from multiagent models (or Distributed AI), which simulate societies of artificial agents.
Situated Agents: These models move beyond the "isolated" cognition of early AI to explore how social structure, communication, and interaction shape behavior.
Emergence: Carley emphasizes that complex social phenomena (norms, cooperation, hierarchies) can "emerge from the interactions among even simple agents". For example, simple rules about learning and interaction can lead to the spontaneous development of social stability or stratification without explicit programming.
The "Socialness" of Agents: Carley, drawing on her work with Newell, argues that "socialness" arises from cognitive limitations. An "omnipotent agent" has no need for society; agents become social because they are boundedly rational and must rely on others for information and survival.
Cognitive Requirements for Social Agents Carley proposes a framework for classifying artificial agents based on their cognitive capabilities and knowledge requirements. She argues that to truly simulate social behavior, agents need specific types of knowledge:
Social Knowledge: Knowledge of other agents' models, goals, and social positions.
Historical/Cultural Knowledge: Knowledge of past interactions and shared norms.
Cognitive Limitations: Agents must be restricted in processing power or memory to necessitate social reliance. Agents that are "too cognitively capable have no need to engage in certain actions that social agents engage in".
Conclusion: The Artificial Social Laboratory Carley concludes that computational modeling allows sociologists to conduct "virtual experiments" that are impossible in the real world. By manipulating agent capabilities and structural constraints, researchers can isolate the fundamental mechanisms of social life, creating a "Drosophila" (fruit fly) for sociology—a simple, controllable system for studying complex evolutionary processes. This approach helps refine social theory by forcing researchers to be explicit about their assumptions regarding human cognition and interaction.
15. Li, Cheng, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. 2024. “CultureLLM: Incorporating Cultural Differences into Large Language Models.” arXiv:2402.10946. Preprint, arXiv, December 3. https://doi.org/10.48550/arXiv.2402.10946.
The Thesis: Cost-Effective Cultural Adaptation Li et al. propose "CultureLLM," a cost-effective framework to address the prevalent issue of cultural bias in Large Language Models (LLMs), which often favor Western perspectives due to the dominance of English training data. Instead of relying on expensive pre-training or often ineffective prompt engineering, the authors demonstrate that LLMs can be fine-tuned to align with specific cultural values using a small set of high-quality seed data augmented through semantic generation.
Methodology: Semantic Data Augmentation The core innovation is a pipeline that transforms a small seed dataset into a robust training corpus:
Seed Data: The authors utilize the World Value Survey (WVS), selecting 50 questions covering seven topics (e.g., social values, security, migration) to serve as the ground truth for cultural attitudes.
Augmentation Process: To prevent overfitting on such a small dataset, they employ a two-step semantic data augmentation technique. First, they use GPT-4 to generate semantically equivalent but stylistically diverse templates of the seed questions. Second, they generate "intact samples" by replacing context-aware synonyms within those templates, filtering the results to ensure semantic preservation.
Fine-Tuning: Using this augmented data, they fine-tune both culture-specific models (e.g., CultureLLM-Ar for Arabic) and a unified model (CultureLLM-One) capable of handling multiple cultures simultaneously.
Evaluation and Results The researchers evaluated CultureLLM across 9 cultures (Arabic, Bengali, Chinese, English, German, Korean, Portuguese, Spanish, and Turkish) using 60 datasets covering tasks like hate speech detection, offensive language detection, and stance detection.
Performance: CultureLLM significantly outperformed baseline models, exceeding GPT-3.5 by 8.1% and Gemini Pro by 9.5% on average F1 scores, achieving performance comparable to or better than GPT-4.
Human Verification: A human study confirmed that the augmented training samples maintained 96.5% semantic equivalence to the original WVS seeds, validating the quality of the generation pipeline.
Key Insights
English vs. Native Data: The study found that fine-tuning models on English data (imbued with specific cultural values) often outperformed fine-tuning on translated data in the native language. This suggests that leveraging the model's inherent strength in English is a viable strategy for low-resource cultural adaptation.
No Catastrophic Forgetting: Experiments on general benchmarks like Big-Bench Hard (BBH) and GSM8K (math) demonstrated that CultureLLM retained its general reasoning capabilities, and in some cases even improved them, suggesting a synergy between cultural knowledge and general reasoning.
Low-Resource Empowerment: The approach offers a scalable solution for "low-resource cultures" where massive training data is unavailable, allowing for the democratization of culturally aware AI without prohibitive computational costs.




